{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimulatedFurutaPendulum{Float64, Random._GLOBAL_RNG}()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Flux: params\n",
    "using ReinforcementLearning\n",
    "using ReinforcementLearningExperiments\n",
    "using Flux\n",
    "import Flux.params\n",
    "using Flux.Losses\n",
    "using IntervalSets\n",
    "using StableRNGs\n",
    "using Flux.Optimise\n",
    "using Flux: crossentropy, ADAM\n",
    "using LinearAlgebra\n",
    "import Base.rand\n",
    "import Random: Sampler, SamplerSimple, Repetition, eltype\n",
    "import Random: seed!\n",
    "using CUDA\n",
    "include(\"../model/FurutaPendulums/ATRXR/src/FurutaPendulums.jl\")\n",
    "using .FurutaPendulums\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FPendulum"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct FPendulum <: AbstractEnv\n",
    "    reward::Float32 = 0.0 # step reward\n",
    "    state::Vector{Float64} = measure(furuta) # Measures the state of the pendulum\n",
    "    action\n",
    "    action_space\n",
    "    state_space\n",
    "    t::Int = 0 # Indicates the number of steps taken\n",
    "    dt\n",
    "    furuta\n",
    "end\n",
    "Main.FPendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLBase.action_space(env::FPendulum) = Space(repeat([ClosedInterval{Float32}(-5.0,5.0)],length(1))) # The input voltage\n",
    "RLBase.reward(env::FPendulum) = env.reward\n",
    "RLBase.state(env::FPendulum) = env.x\n",
    "A = Space(repeat([ClosedInterval{Float64}(-π, π)],length(1)))\n",
    "B = Space(repeat([ClosedInterval{Float64}(-Inf, Inf)],length(1)))\n",
    "RLBase.state_space(env::FPendulum) = [A, B, A, B]\n",
    "RLBase.is_terminated(env::FPendulum) = env.time >= 250 # The episode is terminated after 250 steps\n",
    "function RLBase.reset!(env::FPendulum) # Reset the environment\n",
    "    furuta = SimulatedFurutaPendulum()\n",
    "    control(furuta, 0.0)\n",
    "    env.reward = 0.0\n",
    "    env.t = 0.0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (x::FPendulum)(action::Float32)\n",
    "    control(furuta, action) # Apply the action\n",
    "    env.x = measure(furuta) # Measure the state\n",
    "    env.reward = -abs(env.x[3]) - abs(env.x[4]) # Calculate the reward\n",
    "    env.t += 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# FPendulum\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                  Value |\n",
       "|:----------------- | ----------------------:|\n",
       "| NumAgentStyle     |          SingleAgent() |\n",
       "| DynamicStyle      |           Sequential() |\n",
       "| InformationStyle  | ImperfectInformation() |\n",
       "| ChanceStyle       |           Stochastic() |\n",
       "| RewardStyle       |           StepReward() |\n",
       "| UtilityStyle      |           GeneralSum() |\n",
       "| ActionStyle       |     MinimalActionSet() |\n",
       "| StateStyle        |     Observation{Any}() |\n",
       "| DefaultStateStyle |     Observation{Any}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## State Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float64}}}[Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[-3.141592653589793..3.141592653589793]), Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[-Inf..Inf]), Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[-3.141592653589793..3.141592653589793]), Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[-Inf..Inf])]`\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float32}}}(ClosedInterval{Float32}[-5.0..5.0])`\n",
       "\n",
       "## Current State\n",
       "\n",
       "```\n",
       "[-0.008489333402506015, -0.005777222460162207, 3.1433747083208754, -0.00044036602123035876]\n",
       "```\n"
      ],
      "text/plain": [
       "# FPendulum\n",
       "\n",
       "## Traits\n",
       "\n",
       "| Trait Type        |                  Value |\n",
       "|:----------------- | ----------------------:|\n",
       "| NumAgentStyle     |          SingleAgent() |\n",
       "| DynamicStyle      |           Sequential() |\n",
       "| InformationStyle  | ImperfectInformation() |\n",
       "| ChanceStyle       |           Stochastic() |\n",
       "| RewardStyle       |           StepReward() |\n",
       "| UtilityStyle      |           GeneralSum() |\n",
       "| ActionStyle       |     MinimalActionSet() |\n",
       "| StateStyle        |     Observation{Any}() |\n",
       "| DefaultStateStyle |     Observation{Any}() |\n",
       "\n",
       "## Is Environment Terminated?\n",
       "\n",
       "No\n",
       "\n",
       "## Action Space\n",
       "\n",
       "`Space{Vector{ClosedInterval{Float32}}}(ClosedInterval{Float32}[-5.0..5.0])`\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = FPendulum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can use furuta.x to to measure the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "furuta.x = [0.0, 0.0, 3.141592653589793, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0\n",
       " 3.141592653589793\n",
       " 0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure(furuta)\n",
    "@show furuta.x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD3 Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function RL.Experiment(\n",
    "    ::Val{:JuliaRL},\n",
    "    ::Val{:TD3},\n",
    "    ::Val{:Pendulum},\n",
    "    ::Nothing;\n",
    "    seed = 123,\n",
    ")\n",
    "    # params = furuta.params\n",
    "    env = FPendulum()\n",
    "    rng = StableRNG(seed)\n",
    "    init = glorot_uniform(rng)\n",
    "    ns = 4\n",
    "\n",
    "    # Choosing the actor networks architecture\n",
    "    create_actor() = Chain(\n",
    "        Dense(ns, 30, relu; init = init),\n",
    "        Dense(30, 30, relu; init = init),\n",
    "        Dense(30, 1, tanh; init = init),\n",
    "    ) |> cpu # Change to gpu if you have a gpu\n",
    "\n",
    "    # Choosing the critic networks architecture\n",
    "    create_critic_model() = Chain(\n",
    "        Dense(ns + 1, 30, relu; init = init),\n",
    "        Dense(30, 30, relu; init = init),\n",
    "        Dense(30, 1; init = init),\n",
    "    ) |> cpu # Change to gpu if you have a gpu\n",
    "\n",
    "    create_critic() = TD3Critic(create_critic_model(), create_critic_model())\n",
    "\n",
    "    agent = Agent(\n",
    "        policy = TD3Policy(\n",
    "            behavior_actor = NeuralNetworkApproximator(\n",
    "                model = create_actor(),\n",
    "                optimizer = ADAM(),\n",
    "            ),\n",
    "            behavior_critic = NeuralNetworkApproximator(\n",
    "                model = create_critic(),\n",
    "                optimizer = ADAM(),\n",
    "            ),\n",
    "            target_actor = NeuralNetworkApproximator(\n",
    "                model = create_actor(),\n",
    "                optimizer = ADAM(),\n",
    "            ),\n",
    "            target_critic = NeuralNetworkApproximator(\n",
    "                model = create_critic(),\n",
    "                optimizer = ADAM(),\n",
    "            ),\n",
    "            γ = 0.99f0,\n",
    "            ρ = 0.99f0,\n",
    "            batch_size = 64,\n",
    "            start_steps = 1000,\n",
    "            start_policy = RandomPolicy(-5.0..5.0; rng = rng),\n",
    "            update_after = 1000,\n",
    "            update_freq = 1,\n",
    "            policy_freq = 2,\n",
    "            target_act_limit = 1.0,\n",
    "            target_act_noise = 0.1,\n",
    "            act_limit = 1.0,\n",
    "            act_noise = 0.1,\n",
    "            rng = rng,\n",
    "        ),\n",
    "        trajectory = CircularArraySARTTrajectory(\n",
    "            capacity = 10_000,\n",
    "            state = Vector{Float32} => (ns,),\n",
    "            action = Float32 => (),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    stop_condition = StopAfterStep(10_000, is_show_progress=!haskey(ENV, \"CI\"))\n",
    "    hook = TotalRewardPerEpisode()\n",
    "    Experiment(agent, env, stop_condition, hook, \"# Play Pendulum with TD3\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\section{Play Pendulum with TD3}\n"
      ],
      "text/markdown": [
       "# Play Pendulum with TD3\n"
      ],
      "text/plain": [
       "\u001b[1m  Play Pendulum with TD3\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LoadError",
     "evalue": "method not implemented",
     "output_type": "error",
     "traceback": [
      "method not implemented",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base .\\error.jl:35",
      " [2] (::FPendulum)(action::Float64, player::DefaultPlayer) (repeats 2 times)",
      "   @ ReinforcementLearningBase .\\none:0",
      " [3] _run(policy::Agent{TD3Policy{NeuralNetworkApproximator{Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, ADAM}, NeuralNetworkApproximator{TD3Critic, ADAM}, NeuralNetworkApproximator{Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, ADAM}, NeuralNetworkApproximator{TD3Critic, ADAM}, RandomPolicy{ClosedInterval{Float64}, StableRNGs.LehmerRNG}, StableRNGs.LehmerRNG}, CircularArraySARTTrajectory{NamedTuple{(:state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}, CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}, CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}}}}}, env::FPendulum, stop_condition::StopAfterStep{ProgressMeter.Progress}, hook::TotalRewardPerEpisode)",
      "   @ ReinforcementLearningCore C:\\Users\\Hashim\\.julia\\packages\\ReinforcementLearningCore\\W9FKX\\src\\core\\run.jl:32",
      " [4] run(policy::Agent{TD3Policy{NeuralNetworkApproximator{Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, ADAM}, NeuralNetworkApproximator{TD3Critic, ADAM}, NeuralNetworkApproximator{Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(tanh), Matrix{Float32}, Vector{Float32}}}}, ADAM}, NeuralNetworkApproximator{TD3Critic, ADAM}, RandomPolicy{ClosedInterval{Float64}, StableRNGs.LehmerRNG}, StableRNGs.LehmerRNG}, CircularArraySARTTrajectory{NamedTuple{(:state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}, CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}, CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}}}}}, env::FPendulum, stop_condition::StopAfterStep{ProgressMeter.Progress}, hook::TotalRewardPerEpisode)",
      "   @ ReinforcementLearningCore C:\\Users\\Hashim\\.julia\\packages\\ReinforcementLearningCore\\W9FKX\\src\\core\\run.jl:10",
      " [5] run(x::Experiment; describe::Bool)",
      "   @ ReinforcementLearningCore C:\\Users\\Hashim\\.julia\\packages\\ReinforcementLearningCore\\W9FKX\\src\\core\\experiment.jl:56",
      " [6] run(x::Experiment)",
      "   @ ReinforcementLearningCore C:\\Users\\Hashim\\.julia\\packages\\ReinforcementLearningCore\\W9FKX\\src\\core\\experiment.jl:54",
      " [7] top-level scope",
      "   @ In[10]:4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both Losses and NNlib export \"ctc_loss\"; uses of it in module Flux must be qualified\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "using Plots\n",
    "ex = E`JuliaRL_TD3_Pendulum`\n",
    "run(ex)\n",
    "@show plot(ex.hook.rewards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = StableRNG(123)\n",
    "RLBase.test_runnable!(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env |> action_space |> rand |> env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(RandomPolicy(action_space(env)), env, StopAfterEpisode(1_000))\n",
    "EmptyHook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = RandomPolicy([action_space(env)])\n",
    "ts, rs, xs = Int[], Float32[], Float64[]\n",
    "for i = 1:2000\n",
    "    env |> policy |> env\n",
    "    # write your own logic here\n",
    "    # like saving parameters, recording loss function, evaluating policy, etc.\n",
    "    push!(ts, env.time)\n",
    "    push!(rs, env.reward)\n",
    "    push!(xs, env.x[1])\n",
    "#    is_terminated(env) && reset!(env)\n",
    "end\n",
    "plot(ts, xs, xlabel = \"time\", ylabel = \"x\", legend = false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
