{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ReinforcementLearning\n",
    "using Flux\n",
    "using Flux.Losses\n",
    "using IntervalSets\n",
    "using StableRNGs\n",
    "using ControlSystems, FurutaPendulums\n",
    "using BSON\n",
    "using Logging\n",
    "using TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct pidEnv <: AbstractEnv\n",
    "    state\n",
    "    reward::AbstractFloat\n",
    "    action_space\n",
    "    state_space\n",
    "    done\n",
    "    furuta\n",
    "    t\n",
    "    dt\n",
    "    tmax\n",
    "    last_time\n",
    "    Beta\n",
    "    K\n",
    "    N\n",
    "    Td\n",
    "    Ti\n",
    "    Tr\n",
    "    yOld\n",
    "    eOld\n",
    "    e\n",
    "    I\n",
    "    D\n",
    "    u\n",
    "end\n",
    "\n",
    "function pidEnv(;\n",
    "        max_u=[3,5,10,10,10,4],\n",
    "        min_u=[0, 0, 0, 0, 0, 0],\n",
    "        max_dθ=1e10,\n",
    "        max_dϕ=1e10,\n",
    "        dt = 0.01,\n",
    "        tmax = 5.\n",
    "        )\n",
    "        high = [2*pi,max_dθ,2*pi,max_dϕ]\n",
    "        low = [0,-max_dθ,0,-max_dϕ]\n",
    "            furuta = SimulatedFurutaPendulum()\n",
    "    pidEnv(\n",
    "        measure(furuta),\n",
    "        0.,\n",
    "        Space(ClosedInterval{Float64}.(min_u,max_u)),\n",
    "        Space(ClosedInterval{Float64}.(low, high)),\n",
    "        false,\n",
    "        furuta,\n",
    "        0.,\n",
    "        dt,\n",
    "        tmax,\n",
    "        time(),\n",
    "        1.,\n",
    "        1.,\n",
    "        7.,\n",
    "        1.,\n",
    "        0.,\n",
    "        10.,\n",
    "        0.,\n",
    "        0.,\n",
    "        0.,\n",
    "        0.,\n",
    "        0.,\n",
    "        0.\n",
    "            )\n",
    "end\n",
    "\n",
    "RLBase.action_space(env::pidEnv) = env.action_space\n",
    "RLBase.state_space(env::pidEnv) = env.state_space\n",
    "RLBase.reward(env::pidEnv) = env.reward\n",
    "function _reward(env::pidEnv)\n",
    "    ϕ, ϕdot, θ, θdot = measure(env.furuta)\n",
    "    costs = 1 - cos(θ)\n",
    "    return -costs\n",
    "end\n",
    "RLBase.is_terminated(env::pidEnv) = env.done\n",
    "function RLBase.state(env::pidEnv)\n",
    "    ϕ, ϕdot, θ, θdot = measure(env.furuta)\n",
    "    [ϕ,ϕdot,θ,θdot]\n",
    "end\n",
    "\n",
    "function (env::pidEnv)(a::Vector{AbstractFloat})\n",
    "    @assert a in env.action_space\n",
    "    @show a\n",
    "    env.Beta, env.K, env.N, env.Td, env.Ti, env.Tr = a\n",
    "\n",
    "    y = cos(env.state[3])\n",
    "    env.e = 1 - y\n",
    "    ad = env.Td/(env.N*env.dt + env.Td) \n",
    "    bd = env.K*env.N*ad\n",
    "    env.D = ad*env.D + bd*(y-env.eOld)\n",
    "    v = env.K*(env.Beta-env.e) + env.I + env.D\n",
    "    env.u = clamp(v,-5,5)\n",
    "\n",
    "    control(env.furuta,env.u)\n",
    "    env.last_time = periodic_wait(env.furuta,env.t,env.dt)\n",
    "    env.t += env.dt\n",
    "    env.done = env.t >= env.tmax\n",
    "\n",
    "    if env.Ti == 0\n",
    "        I = 0.0;\n",
    "    else \n",
    "        I = I + (env.K*env.dt/env.Ti)*env.e + (env.dt/env.Tr)*(env.u - v);\n",
    "    end\n",
    "    env.yOld = y;\n",
    "    env.eOld = env.e;\n",
    "end\n",
    "\n",
    "function RLBase.reset!(env::pidEnv)\n",
    "    env.last_time = time()\n",
    "    env.reward = 0.\n",
    "    env.t = 0.\n",
    "    env.furuta = SimulatedFurutaPendulum()\n",
    "    env.state = measure(env.furuta)\n",
    "    env.I = 0.0\n",
    "    env.yOld = 0.0\n",
    "    env.eOld = 0.0\n",
    "    env.done = false\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIDNeuralNet(Chain(Dense(5, 30, relu), Dense(30, 6, tanh)), pidEnv([6.010434288348817, 0.0, 3.141592653589793, 0.0], 0.0, -5.0..5.0, Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[0.0..6.283185307179586, -1.0e10..1.0e10, 0.0..6.283185307179586, -1.0e10..1.0e10]), false, SimulatedFurutaPendulum{Float64, Random._GLOBAL_RNG}(), 0.0, 0.01, 5.0, 1.685572934208e9, 1.0, 1.0, 7.0, 1.0, 0.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct PIDNeuralNet\n",
    "    chain::Chain\n",
    "end\n",
    "function (m::PIDNeuralNet)(x)\n",
    "    return sum(m.chain(x).*x,dims=1)\n",
    "end\n",
    "\n",
    "Flux.@functor PIDNeuralNet\n",
    "\n",
    "# _init = glorot_uniform(StableRNG(123))\n",
    "function create_actor()\n",
    "    chain = Chain(\n",
    "        Dense(5, 30, relu),\n",
    "        # Dense(30, 30, relu; init = init),\n",
    "        Dense(30, 5, tanh),\n",
    "    )\n",
    "    model = PIDNeuralNet(chain)\n",
    "    return model |> cpu\n",
    "end\n",
    "\n",
    "model = create_actor()\n",
    "batch = [1 1 1 1 1;\n",
    "    1 2 1 2 1]'\n",
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\section{Play Pendulum with TD3}\n"
      ],
      "text/markdown": [
       "# Play Pendulum with TD3\n"
      ],
      "text/plain": [
       "\u001b[1m  Play Pendulum with TD3\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access 6-element Vector{Float64} at index []",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 6-element Vector{Float64} at index []",
      "",
      "Stacktrace:",
      "  [1] throw_boundserror(A::Vector{Float64}, I::Tuple{})",
      "    @ Base .\\abstractarray.jl:703",
      "  [2] checkbounds",
      "    @ .\\abstractarray.jl:668 [inlined]",
      "  [3] _getindex",
      "    @ .\\abstractarray.jl:1273 [inlined]",
      "  [4] getindex(::Vector{Float64})",
      "    @ Base .\\abstractarray.jl:1241",
      "  [5] (::TD3Policy{NeuralNetworkApproximator{PIDNeuralNet, ADAM}, NeuralNetworkApproximator{TD3Critic, ADAM}, NeuralNetworkApproximator{PIDNeuralNet, ADAM}, NeuralNetworkApproximator{TD3Critic, ADAM}, RandomPolicy{ClosedInterval{Float64}, StableRNGs.LehmerRNG}, StableRNGs.LehmerRNG})(env::pidEnv)",
      "    @ ReinforcementLearningZoo C:\\Users\\Hashim\\.julia\\packages\\ReinforcementLearningZoo\\tvfq9\\src\\algorithms\\policy_gradient\\td3.jl:125",
      "  [6] Agent",
      "    @ C:\\Users\\Hashim\\.julia\\packages\\ReinforcementLearningCore\\W9FKX\\src\\policies\\agents\\agent.jl:24 [inlined]",
      "  [7] _run(policy::Agent{TD3Policy{NeuralNetworkApproximator{PIDNeuralNet, ADAM}, NeuralNetworkApproximator{TD3Critic, ADAM}, NeuralNetworkApproximator{PIDNeuralNet, ADAM}, NeuralNetworkApproximator{TD3Critic, ADAM}, RandomPolicy{ClosedInterval{Float64}, StableRNGs.LehmerRNG}, StableRNGs.LehmerRNG}, CircularArraySARTTrajectory{NamedTuple{(:state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}, CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}, CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}}}}}, env::pidEnv, stop_condition::StopAfterEpisode{ProgressMeter.Progress}, hook::TotalRewardPerEpisode)",
      "    @ ReinforcementLearningCore C:\\Users\\Hashim\\.julia\\packages\\ReinforcementLearningCore\\W9FKX\\src\\core\\run.jl:27",
      "  [8] run(policy::Agent{TD3Policy{NeuralNetworkApproximator{PIDNeuralNet, ADAM}, NeuralNetworkApproximator{TD3Critic, ADAM}, NeuralNetworkApproximator{PIDNeuralNet, ADAM}, NeuralNetworkApproximator{TD3Critic, ADAM}, RandomPolicy{ClosedInterval{Float64}, StableRNGs.LehmerRNG}, StableRNGs.LehmerRNG}, CircularArraySARTTrajectory{NamedTuple{(:state, :action, :reward, :terminal), Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}, CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}, CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}, CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}}}}}, env::pidEnv, stop_condition::StopAfterEpisode{ProgressMeter.Progress}, hook::TotalRewardPerEpisode)",
      "    @ ReinforcementLearningCore C:\\Users\\Hashim\\.julia\\packages\\ReinforcementLearningCore\\W9FKX\\src\\core\\run.jl:10",
      "  [9] run(x::Experiment; describe::Bool)",
      "    @ ReinforcementLearningCore C:\\Users\\Hashim\\.julia\\packages\\ReinforcementLearningCore\\W9FKX\\src\\core\\experiment.jl:56",
      " [10] run(x::Experiment)",
      "    @ ReinforcementLearningCore C:\\Users\\Hashim\\.julia\\packages\\ReinforcementLearningCore\\W9FKX\\src\\core\\experiment.jl:54",
      " [11] top-level scope",
      "    @ In[43]:86"
     ]
    }
   ],
   "source": [
    "function RL.Experiment(\n",
    "    ::Val{:JuliaRL},\n",
    "    ::Val{:TD3},\n",
    "    ::Val{:Pendulum},\n",
    "    ::Nothing;\n",
    "    seed = 123,\n",
    ")\n",
    "    rng = StableRNG(seed)\n",
    "    env = pidEnv()\n",
    "    A = action_space(env)\n",
    "    ns = length(state(env))\n",
    "    init = glorot_uniform(rng)\n",
    "    na = length(A)\n",
    "\n",
    "    create_actor() = Chain(\n",
    "        Dense(ns, 32, relu; init = init),\n",
    "        Dense(32, 32, relu; init = init),\n",
    "        Dense(32, na, tanh; init = init),\n",
    "    ) |> cpu\n",
    "\n",
    "    create_critic_model() = Chain(\n",
    "        Dense(ns + na, 32, relu; init = init),\n",
    "        Dense(32, 32, relu; init = init),\n",
    "        Dense(32, 1; init = init),\n",
    "    ) |> cpu\n",
    "\n",
    "    create_critic() = TD3Critic(create_critic_model(), create_critic_model())\n",
    "\n",
    "    agent = Agent(\n",
    "        policy = TD3Policy(\n",
    "            behavior_actor = NeuralNetworkApproximator(\n",
    "                model = create_actor(),\n",
    "                optimizer = ADAM(),\n",
    "            ),\n",
    "            behavior_critic = NeuralNetworkApproximator(\n",
    "                model = create_critic(),\n",
    "                optimizer = ADAM(),\n",
    "            ),\n",
    "            target_actor = NeuralNetworkApproximator(\n",
    "                model = create_actor(),\n",
    "                optimizer = ADAM(),\n",
    "            ),\n",
    "            target_critic = NeuralNetworkApproximator(\n",
    "                model = create_critic(),\n",
    "                optimizer = ADAM(),\n",
    "            ),\n",
    "            γ = 0.9f0,\n",
    "            ρ = 0.99f0,\n",
    "            batch_size = 64,\n",
    "            start_steps = 1000,\n",
    "            start_policy = RandomPolicy({0. for i in 1:6}..{5. for i in 1:6}; rng = rng),\n",
    "            update_after = 1000,\n",
    "            update_freq = 1,\n",
    "            policy_freq = 2,\n",
    "            target_act_limit = 5.0,\n",
    "            target_act_noise = 0.1,\n",
    "            act_limit = 5.0,\n",
    "            act_noise = 0.1,\n",
    "            rng = rng,\n",
    "        ),\n",
    "        trajectory = CircularArraySARTTrajectory(\n",
    "            capacity = 1_500_000,\n",
    "            state = Vector{Float32} => (ns,),\n",
    "            action = Vector{AbstractFloat} => (na,),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    #stop_condition = StopAfterStep(10000, is_show_progress=!haskey(ENV, \"CI\"))\n",
    "    stop_condition = StopAfterEpisode(5; cur = 0, is_show_progress = true)\n",
    "    hook = TotalRewardPerEpisode() \n",
    "    Experiment(agent, env, stop_condition, hook, \"# Play Pendulum with TD3\")\n",
    "end\n",
    "\n",
    "using Plots\n",
    "pid = E`JuliaRL_TD3_Pendulum`\n",
    "run(pid)\n",
    "plot(ex.hook.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "method not implemented",
     "output_type": "error",
     "traceback": [
      "method not implemented",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base .\\error.jl:35",
      " [2] (::pidEnv)(action::Vector{Float64}, player::DefaultPlayer) (repeats 2 times)",
      "   @ ReinforcementLearningBase .\\none:0",
      " [3] _run(policy::RandomPolicy{Nothing, Random._GLOBAL_RNG}, env::pidEnv, stop_condition::StopAfterEpisode{ProgressMeter.Progress}, hook::TotalRewardPerEpisode)",
      "   @ ReinforcementLearningCore C:\\Users\\Hashim\\.julia\\packages\\ReinforcementLearningCore\\W9FKX\\src\\core\\run.jl:32",
      " [4] run(policy::RandomPolicy{Nothing, Random._GLOBAL_RNG}, env::pidEnv, stop_condition::StopAfterEpisode{ProgressMeter.Progress}, hook::TotalRewardPerEpisode)",
      "   @ ReinforcementLearningCore C:\\Users\\Hashim\\.julia\\packages\\ReinforcementLearningCore\\W9FKX\\src\\core\\run.jl:10",
      " [5] top-level scope",
      "   @ In[9]:1"
     ]
    }
   ],
   "source": [
    "run(\n",
    "           RandomPolicy(),\n",
    "        #    ex.policy,\n",
    "        #    model,\n",
    "           pidEnv(),\n",
    "           StopAfterEpisode(10),\n",
    "           TotalRewardPerEpisode()\n",
    "       )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
