{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c997d48-5ffd-47e8-ace6-51fd56570f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base: +, *\n",
    "using Flux\n",
    "import Flux: params\n",
    "using Flux.Optimise\n",
    "using Flux: crossentropy, ADAM\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "import ProgressMeter\n",
    "import ProgressMeter: @showprogress\n",
    "PM = ProgressMeter\n",
    "PM.ijulia_behavior(:clear)\n",
    "using Statistics\n",
    "import Random: seed!\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49bc20-5df1-468c-aeb0-16f6920383da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the reward function. You can adjust it however you like.\n",
    "# Choosing your reward wisely otherwise it might not result in the intended behavior.\n",
    "stage_reward(x; sp, ap) = Δt*( - x[3]*ap - x[4]*ifelse(x[3] < π/2, 1, -sp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82093651-bf8f-4a73-9fa1-c34f8e6eeb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can used during training to explore the environment and generate the data.\n",
    "function (x::LotteryEnv)(action)\n",
    "    x = x.state\n",
    "    x₊ = sim(x, action)\n",
    "    x.reward = (- x₊[3]*ap - x₊[4]*ifelse(x₊[3] < π/2, 1, -sp)\n",
    "    return x₊, reward\n",
    "end\n",
    "end\n",
    "\n",
    "function play(p :: Net; x_init, n_steps, cp, sp, ϵ)\n",
    "    x = x_init\n",
    "    for k in 1:n_steps\n",
    "        if rand() > ϵ\n",
    "            u = p(x)\n",
    "        else\n",
    "            u = rand()*10 - 5\n",
    "        end\n",
    "        x₊ = sim(x,u)\n",
    "        sc = stage_reward(x₊, cp=cp, sp=sp)\n",
    "        push!(replay_buffer, [x=x, u=u, x₊=x₊, sc=sc])\n",
    "        x = x₊\n",
    "    end\n",
    "    return replay_buffer\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10332a98-51dc-4794-9713-1df27002ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function runs the policy for n_steps and returns the trajectory and total reward.\n",
    "function run(p :: Net; x_init, n_steps)\n",
    "    x = x_init\n",
    "    xs = []\n",
    "    c = 0\n",
    "    for k in 1:n_steps\n",
    "        u = p(x)\n",
    "        x = sim(x,u)\n",
    "        push!(xs, x)\n",
    "        c += stage_reward(x, cp=cp, sp=sp)\n",
    "    end\n",
    "    xs, c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1879d492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log[1:end, 1:2] = [200 201; 200 201; 200 201; 200 201]\n",
      "log[1:end, 4] = [4, 4, 4, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Int64}:\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function calculates the gradient of the policy with respect to the Q function for a given state.\n",
    "function policy_gradient(x, p, q)\n",
    "    u = p(x)\n",
    "    gs = gradient(u -> q(x,u), u)\n",
    "    return grad\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fda455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Q-learning algorithm, which returns the log of the Q values\n",
    "function Q_learning(x_init=[0,0,0,0], p, steps=500, γ=0.5, α=0.9, ϵ-0.5, log=[])\n",
    "    for i in 1:steps\n",
    "        if rand() > ϵ\n",
    "            u = p(x)\n",
    "        else\n",
    "            u = rand()*10 - 5\n",
    "        end\n",
    "        x₊ = sim(x,u)\n",
    "        reward = stage_reward(x₊, cp=cp, sp=sp)\n",
    "        x, u = round(x), round(u)\n",
    "        q_values[x,u] += γ * (reward + α*np.max(q_values[round(x₊)]) - q_values[x, u])\n",
    "    end\n",
    "    return q_values\n",
    "end\n",
    "\n",
    "# This function estimates the Q value of a state-action pair using a neural network trained on the log generated by Q_learning\n",
    "function Critic(q_values, q :: Net)\n",
    "    # x,u -> Neural network -> estimated Q value\n",
    "    q_train = [ q_values[x,u] for (x,u) in keys(q_values) ] # I think it is supposed be something like this\n",
    "    q_loss = sum((q_y - q(q_x)).^2)\n",
    "    train!(q_loss, params(q), [q_x,q_y], OptimiserChain(WeightDecay(0.42), Adam(0.1)), cb = () -> println(\"critic\")) # OptimiserChain is used to apply L2 regulization\n",
    "    return q_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43690863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the functions above to implement your own RL algorithm\n",
    "# As an example, we implement the DDPG algorithm\n",
    "function RL_MODEL(x_init, n_steps, critic_cost_bound, n_iters, replay_buffer_size, n_samples, p_layers, q_layers, γ, ap, sp)\n",
    "    qs = []\n",
    "    qs_target = []\n",
    "    ps = []\n",
    "    ps_target = []\n",
    "    \n",
    "    # Initialize the replay buffer\n",
    "    replay_buffer = []\n",
    "    # Initialize the critic and actor networks\n",
    "    p = Net(p_layers)\n",
    "    q = Net(q_layers)\n",
    "    # Initialize the target networks\n",
    "    p̂ = deepcopy(p)\n",
    "    q̂ = deepcopy(q)\n",
    "    # Initialize the optimizer\n",
    "    opt_p = ADAM(0.001)\n",
    "    opt_q = ADAM(0.001)\n",
    "    # Initialize the critic cost\n",
    "    critic_cost = 0\n",
    "    # Initialize the iteration counter\n",
    "    k = 0\n",
    "    # Initialize the total reward\n",
    "    total_reward = 0\n",
    "    # Pre-training\n",
    "    Q_learning(x_init=x_init, p=, steps=500)\n",
    "\n",
    "    println(\"Building initial replay_buffer\")\n",
    "    for y_init in range(-1, 2, length=5), dy_init in range(-1, 1, length=3)\n",
    "        append!(replay_buffer, play(p, sp=sp, cp=cp, ϵ = ϵ, x_init = [y_init, dy_init]))\n",
    "    end\n",
    "    \n",
    "    # Actor training\n",
    "    while critic_cost < critic_cost_bound && k < n_iters\n",
    "        # Sample a batch of transitions from the replay buffer\n",
    "        batch = sample(replay_buffer, n_samples)\n",
    "        # Calculate the target Q value\n",
    "        q_target = batch[1:end,4] + γ * q̂(batch[1:end,3])\n",
    "        # Update the critic network\n",
    "        gs = gradient(() -> crossentropy(q(batch[1:end,1:2]), q_target), params(q))\n",
    "        update!(opt_q, params(q), gs)\n",
    "        # Update the actor network\n",
    "        gs = gradient(() -> -q(p(batch[1:end,1])), params(p))\n",
    "        update!(opt_p, params(p), gs)\n",
    "        # Update the target networks\n",
    "        p̂ = deepcopy(p)\n",
    "        q̂ = deepcopy(q)\n",
    "        # Update the critic cost\n",
    "        critic_cost = crossentropy(q(batch[1:end,1:2]), q_target)\n",
    "        # Update the iteration counter\n",
    "        k += 1\n",
    "    end\n",
    "    # Play the game\n",
    "    log = play(p; x_init, n_steps, ap, sp, ϵ=0.0)\n",
    "    # Calculate the total reward\n",
    "    total_reward = sum([log[i].sc for i in 1:n_steps])\n",
    "    return p, q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the hyperparameters\n",
    "x_init = [0.0, 0.0, 0.0, 0.0]\n",
    "q_layers = Chain(Dense(2, 4, relu), Dense(4, 2, relu), Dense(2, 2), softmax)\n",
    "p_layers = Chain(Dense(2, 4, relu), Dense(4, 2, relu), Dense(2, 2), softmax)\n",
    "n_iters = 40\n",
    "n_steps = 1000\n",
    "n_critic_pre_steps = 100\n",
    "n_samples = 80\n",
    "replay_buffer_size = 100_000\n",
    "critic_cost_bound = 1000\n",
    "γ = 0.05\n",
    "ap = 1\n",
    "sp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "p, q = RL_MODEL(x_init, n_steps, critic_cost_bound, n_iters, replay_buffer_size, n_samples, p_layers, q_layers, γ, ap, sp)\n",
    "\n",
    "# Save the model\n",
    "using BSON\n",
    "@save \"Actor.bson\" p\n",
    "@save \"Critic.bson\" q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074fc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the final model\n",
    "xs, c = run(p, x_init=[0,0,0,0], n_steps=2000)\n",
    "plot(xs[1,:], xs[2,:], label=\"trajectory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
